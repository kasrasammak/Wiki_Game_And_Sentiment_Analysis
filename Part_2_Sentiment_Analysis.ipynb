{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3147: DtypeWarning: Columns (4,5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/owlthekasra/Documents/Code/Python/WikiNLP/Sentiment_Analysis_Dataset.csv', encoding=\"ISO-8859-1\")\n",
    "# df = df.iloc[0::100, :] # make training set easier to work with\n",
    "df = df[[\"Sentiment\", \"SentimentSource\",\"SentimentText\"]].reset_index().iloc[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Separate dataframe into positive and negative sentiment observations\n",
    "positive= df[df[\"Sentiment\"]==1][\"SentimentText\"]\n",
    "negative = df[df[\"Sentiment\"]==0][\"SentimentText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Remove stop words to find meaningful word frequencies\n",
    "stop_words = set(stopwords.words('english'))\n",
    "added_words = [\"-\", \"get\", \"going\", \"go\", \"I'm\", \"im\", \"u\",\"know\", \"&amp;\", \"got\", \"I'll\", \"@\", \"that's\", \"like\", \"really\", \"one\", \"...\", \"..\", \"2\", \"?\", \"&lt;3\",\"see\"]\n",
    "stop = list(stop_words)\n",
    "stop.extend(added_words)\n",
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% functions to get determine frequency of words\n",
    "def wordListToFreqDf(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return pd.DataFrame(list(zip(wordlist,wordfreq)))\n",
    "\n",
    "def getMostFrequent(posneg):\n",
    "    temp = [wrd for sub in posneg for wrd in sub.split()]\n",
    "    filt = [w for w in temp if not w.lower() in stop]\n",
    "    freqdf = wordListToFreqDf(filt)\n",
    "    vals = freqdf.sort_values(by=1,ascending=False)\n",
    "    freq = vals.groupby(0).count().sort_values(by=1, ascending=False)\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% find most positive and negative words\n",
    "pos_freq = getMostFrequent(positive).reset_index()\n",
    "neg_freq = getMostFrequent(negative).reset_index()\n",
    "\n",
    "pos_freq = pos_freq[pos_freq[0]!=\"I'm\"].reset_index().iloc[:,1:]\n",
    "neg_freq = neg_freq[neg_freq[0]!=\"I'm\"].reset_index().iloc[:,1:]\n",
    "\n",
    "top_10_pos = pos_freq.iloc[0:10,0:2]\n",
    "top_10_neg = neg_freq.iloc[0:10,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "top_10_pos['rank'] = top_10_pos.index + 1\n",
    "top_10_pos['zipf'] = top_10_pos[1] * top_10_pos['rank']\n",
    "\n",
    "top_10_neg['rank'] = top_10_neg.index + 1\n",
    "top_10_neg['zipf'] = top_10_neg[1] * top_10_neg['rank']\n",
    "\n",
    "# when multiplying the count by the rank, \n",
    "# you do not approach a constant\n",
    "# which may or may not be due to removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(top_10_pos[1], top_10_pos['rank'])\n",
    "sns.lineplot(top_10_neg[1], top_10_neg['rank'])\n",
    "\n",
    "# The inverse proportionality part of zipf law is true, \n",
    "# as you can see a vaguely 1/x to -x graph in the plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Machine Learning Classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Getting rid of symbols from text\n",
    "## from https://stackabuse.com/text-classification-with-python-and-scikit-learn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "documents = []\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create data and class sets\n",
    "X, y = df['SentimentText'], df['Sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Vectorize text\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stop)\n",
    "X = tfidfconverter.fit_transform(documents).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% split into training and test sets (Test = 1/10th of entire set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Test on Random Forest\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "acc_rf = accuracy_score(y_pred, y_test) \n",
    "\n",
    "cv_score_rf = cross_val_score(RandomForestClassifier(n_estimators=1000, random_state=0), X, y, scoring='accuracy', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Test on Naive Bayes Classifier\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred_clf = clf.predict(X_test)\n",
    "\n",
    "acc_nb = accuracy_score(y_pred_clf, y_test)\n",
    "cv_score_nb = cross_val_score(MultinomialNB(), X, y, scoring='accuracy', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Check Kappa\n",
    "def getKappa(test, pred):\n",
    "    cm = confusion_matrix(test, pred)\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    obs = 0\n",
    "    for i in range(0,len(cm)):\n",
    "        num = num + (sum(cm[i])*sum(cm[:,i]))\n",
    "        denom = denom+sum(cm[i])\n",
    "        obs = obs + cm[i,i]\n",
    "    expected = num/denom\n",
    "    kappa = (obs - expected)/(denom - expected)\n",
    "    return kappa\n",
    "\n",
    "kappa_forest = getKappa(y_test,y_pred)\n",
    "kappa_np = getKappa(y_test,y_pred_clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
